{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Classification on Hate Speech\n",
    "\n",
    "---\n",
    "\n",
    "__2nd Semester Data Science Master__  \n",
    "__Beuth University of Applied Sciences Berlin__\n",
    "\n",
    "__by Arndt, Ana, Christian, Ervin, Malte__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Content\n",
    "--- \n",
    "\n",
    "1. Preprocessing\n",
    "1. Baseline\n",
    "1. Pretrained Vectors\n",
    "1. Ensembles\n",
    "1. Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import sklearn.metrics as skm\n",
    "from os import chdir\n",
    "from sklearn import model_selection\n",
    "import skift\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "chdir(\"/home/arndt/git-reps/hatespeech/\")\n",
    "\n",
    "df_test = pd.merge(pd.read_csv(\"data/test.csv\"),\n",
    "                   pd.read_csv(\"data/test_labels.csv\"),\n",
    "                   how=\"inner\",\n",
    "                   on=\"id\")\n",
    "\n",
    "df_train=pd.read_csv(\"data/data_unidecode.csv\")\n",
    "\n",
    "# data preperation\n",
    "\n",
    "df_train=df_train[[\"id\",\"comment_text\",\"toxic\"]]\n",
    "df_train[\"label\"]=\"__label__not_toxic\"\n",
    "df_train.loc[df_train[\"toxic\"]==1,\"label\"]=\"__label__toxic\"\n",
    "df_train[\"comment_text\"]=df_train[\"comment_text\"].apply(str.replace,args=(\"\\n\",\" \"))\n",
    "df_train[\"comment_text\"]=df_train[\"comment_text\"].apply(str.replace,args=(\"\\\"\",\"\"))\n",
    "\n",
    "df_test[\"comment_text\"]=df_test[\"comment_text\"].apply(str.replace,args=(\"\\n\",\" \"))\n",
    "df_test[\"comment_text\"]=df_test[\"comment_text\"].apply(str.replace,args=(\"\\\"\",\"\"))\n",
    "\n",
    "X_train = pd.DataFrame(df_train.loc[:,\"comment_text\"])\n",
    "y_train = df_train.loc[:,\"toxic\"]\n",
    "#-1 means the row wasn't used for scoring in the Kaggle competition\n",
    "X_test = pd.DataFrame(df_test[df_test[\"toxic\"]>-1].loc[:,\"comment_text\"]) \n",
    "y_test = df_test[df_test[\"toxic\"]>-1].loc[:,\"toxic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Preprocessing and Normalization  \n",
    "---  \n",
    "Improve the performance of the model applying some simple pre-processing  \n",
    "- Translation into English \n",
    "- Only ASCII characters (unidecode)\n",
    "- Remove special characters \n",
    "- Change Emojis to words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Google Translation API Requests\n",
    "---\n",
    "\n",
    "![Dashboard](google_dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Replace Characters  \n",
    "---  \n",
    "```py \n",
    "def replacetext(text):\n",
    "    for key, value in REPLACE_TO.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text\n",
    "```\n",
    "\n",
    "```py \n",
    "REPLACE_TO = { \n",
    "':)':'happy' , ':(':'sad', ':P':'funny' ,  \n",
    "'@':'at' , '&':'and' , 'i\\'m':'i am' , 'don\\'t':'do not' , 'can\\'t':'can not' ,   \n",
    "'.':'' , ',':'' , ':':'', ';':'' , '!':'' , '\\'':' ' , '?':' ' , '(':' ', ')':' ' , '[':' ' , ']':' ' , '-':' ' , '#':' ' , '=':' ' , '+':' ' , '/':' ' , '\"':' ' ,   \n",
    "'0':' zero ' , '1':' one ' , '2':' two ' , '3':' three ', '3':' three ' , '4':' four ' , '5':' five ' ,'6' :' six ' , '7':' seven ' , '8':' eight ' , '9':' nine ' }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def score_preds(y_true, y_pred):\n",
    "    print(\"confusion matrix:\")\n",
    "    print(str(skm.confusion_matrix(y_true, y_pred)))\n",
    "    print(\"classification report:\")\n",
    "    print(str(skm.classification_report(y_true, y_pred)))\n",
    "    print(\"f1 macro: %0.4f\" % (skm.precision_recall_fscore_support(y_true, y_pred, average='macro')[2]))\n",
    "    print(\"f1 micro: %0.4f\" % (skm.precision_recall_fscore_support(y_true, y_pred, average='micro')[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Majority class classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[57888     0]\n",
      " [ 6090     0]]\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95     57888\n",
      "          1       0.00      0.00      0.00      6090\n",
      "\n",
      "avg / total       0.82      0.90      0.86     63978\n",
      "\n",
      "f1 macro: 0.4750\n",
      "f1 micro: 0.9048\n"
     ]
    }
   ],
   "source": [
    "score_preds(y_test, np.zeros(y_test.shape)) #set all predictions to non-toxic (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* By only assigning all fitted values to the majority class we get a F1 score of 90%. \n",
    "* This is, because the test dataset is imbalanced and contains only 10% toxic comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Single skift model\n",
    "\n",
    "skift stands for scikit fasttext - scikit-learn wrappers for Python ([GitHub](https://github.com/shaypal5/skift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[55268  2620]\n",
      " [ 2152  3938]]\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.95      0.96     57888\n",
      "          1       0.60      0.65      0.62      6090\n",
      "\n",
      "avg / total       0.93      0.93      0.93     63978\n",
      "\n",
      "f1 macro: 0.7907\n",
      "f1 micro: 0.9254\n",
      "f1 micro on training data: 0.9745\n"
     ]
    }
   ],
   "source": [
    "skift_clf = skift.FirstObjFtClassifier()\n",
    "skift_clf.fit(X_train, y_train)\n",
    "\n",
    "preds = skift_clf.predict(X_test)\n",
    "score_preds(y_test, preds)\n",
    "\n",
    "print(\"f1 micro on training data: %0.4f\" % (skift_clf.score(X_train, y_train))) #overfitted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Single skift model - pretrained vectors\n",
    "\n",
    "fastText English Word Vectors trained on Wikipedia 2017, UMBC webbase corpus, and statmt.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on test data: 0.9294\n",
      "score on training data: 0.9676\n",
      "confusion matrix:\n",
      "[[54933  2955]\n",
      " [ 1564  4526]]\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.95      0.96     57888\n",
      "          1       0.60      0.74      0.67      6090\n",
      "\n",
      "avg / total       0.94      0.93      0.93     63978\n",
      "\n",
      "f1 macro: 0.8138\n",
      "f1 micro: 0.9294\n"
     ]
    }
   ],
   "source": [
    "skift_clf = skift.FirstObjFtClassifier(minn=3, maxn=3, pretrainedVectors=\"data/wiki-news-300d-1M.vec\")\n",
    "skift_clf.fit(X_train, y_train)\n",
    "\n",
    "preds = skift_clf.predict(X_test)\n",
    "score_preds(y_test, preds)\n",
    "\n",
    "print(\"f1 micro on training data: %0.4f\" % (skift_clf.score(X_train, y_train))) #overfitted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733432851735352\n",
      "0.9461546434494196\n",
      "0.7536945812807881\n",
      "0.5955624756714675\n"
     ]
    }
   ],
   "source": [
    "### For non-toxic ###\n",
    "# Recall = TP/(TP+FN)\n",
    "print(54771/(54771+1500))\n",
    "# Precision = TP/(TP+FP)\n",
    "print(54771/(54771+3117))\n",
    "\n",
    "### For toxic ###\n",
    "# Recall = TP/(TP+FN)\n",
    "print(4590/(4590+1500))\n",
    "# Precision = TP/(TP+FP)\n",
    "print(4590/(4590+3117))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Check common errors - false negatives\n",
    "\n",
    "What commonnalities have the false negatives? Check common errors …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "comp = pd.merge(pd.DataFrame({\"comment_text\" : X_test[\"comment_text\"].values, \"toxic_pred\" : preds}),\n",
    "                pd.concat([X_test, y_test], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2152, 3)\n",
      "                                          comment_text  toxic_pred  toxic\n",
      "8    == Arabs are committing genocide in Iraq, but ...         0.0      1\n",
      "29                :Fuck off, you anti-semitic cunt.  |         0.0      1\n",
      "38   How dare you vandalize that page about the HMS...         0.0      1\n",
      "127  :::::::::Moi? Ego? I am mortified that you cou...         0.0      1\n",
      "133      So, on the tenth anniversary of 9/11, New ...         0.0      1\n"
     ]
    }
   ],
   "source": [
    "false_negatives = comp[(comp[\"toxic\"]==1) & (comp[\"toxic_pred\"]==0)]\n",
    "print(false_negatives.shape)\n",
    "false_negatives[\"comment_text\"].to_csv(\"data/false_negatives.txt\")\n",
    "print(false_negatives.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Check common errors - false positives\n",
    "\n",
    "What commonnalities have the false positives? Check common errors …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2620, 3)\n",
      "                                          comment_text  toxic_pred  toxic\n",
      "2    ::: Somebody will invariably try to add Religi...         1.0      0\n",
      "27   I WILL BURN YOU TO HELL IF YOU REVOKE MY TALK ...         1.0      0\n",
      "79                           WHAT THE HELL      Justin         1.0      0\n",
      "99                               and lewd sex in China         1.0      0\n",
      "148      ::: You have my trust. But trust me on thi...         1.0      0\n"
     ]
    }
   ],
   "source": [
    "false_positives = comp[(comp[\"toxic\"]==0) & (comp[\"toxic_pred\"]==1)]\n",
    "print(false_positives.shape)\n",
    "false_positives[\"comment_text\"].to_csv(\"data/false_positives.txt\")\n",
    "print(false_positives.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble predictions\n",
    "\n",
    "Make predictions with a list of classifiers on a dataframe X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def ensemble_predict_proba(classifiers, X):\n",
    "    proba = [classifier.predict_proba(X) for classifier in classifiers]\n",
    "    mean = np.zeros(proba[0].shape)\n",
    "    for i in range(len(classifiers)):\n",
    "        mean = mean + proba[i]\n",
    "    mean = mean / float(len(classifiers))\n",
    "    return mean\n",
    "\n",
    "def ensemble_predict(classifiers, X):\n",
    "    kfold_proba = ensemble_predict_proba(classifiers, X)\n",
    "    kfold_labels = np.zeros(kfold_proba.shape[0]) #initialize array\n",
    "    kfold_labels[kfold_proba[:,0]<=kfold_proba[:,1]] = 1\n",
    "    return kfold_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build multiple models using K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "seed = 77\n",
    "kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=seed) #add variance through randomnization\n",
    "\n",
    "# build multiple models using k folds:\n",
    "kfold_clfs = list()\n",
    "for train_index, test_index in kfold.split(df_train):\n",
    "    X = pd.DataFrame(df_train.loc[:,\"comment_text\"])\n",
    "    y = df_train.loc[:,\"toxic\"]\n",
    "    clf = skift.FirstObjFtClassifier(minn=3, maxn=3, pretrainedVectors=\"data/wiki-news-300d-1M.vec\")\n",
    "    clf.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "    clf.model.quantize()\n",
    "    print(\"Score on test proportion of this fold: %0.3f\" % (clf.score(X.iloc[test_index], y.iloc[test_index])))\n",
    "    kfold_clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "score_preds(y_test, ensemble_predict(kfold_clfs, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# StratifiedKFold\n",
    "\n",
    "* There are different strategies in creating a train set and test set split of your data. \n",
    "* If you want to keep the percentage for each class in each fold the same you want to use a stratified split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "seed = 77\n",
    "stkfold = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "# build multiple models using k folds:\n",
    "stkfold_clfs = list()\n",
    "for train_index, test_index in stkfold.split(X=pd.DataFrame(df_train.loc[:,\"comment_text\"]), \n",
    "                                             y = df_train.loc[:,\"toxic\"]):\n",
    "    clf = skift.FirstObjFtClassifier(minn=3, maxn=3, pretrainedVectors=\"data/wiki-news-300d-1M.vec\")\n",
    "    clf.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "    clf.model.quantize()\n",
    "    print(\"Score on test proportion of this fold: %0.3f\" % (clf.score(X.iloc[test_index], y.iloc[test_index])))\n",
    "    stkfold_clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "score_preds(y_test, ensemble_predict(stkfold_clfs, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KFold Conclusion\n",
    "\n",
    "The main reason we started to use KFold was that we didn't have the labeled test data at the beginning. But after we found the real test data on Kaggle, we used it.\n",
    "\n",
    "* k=10 slightly improved the score on the test set\n",
    "* k=5 scored worse than just a single model on all the training data \n",
    "* StratifiedKFold performed worse than the just KFold.\n",
    "\n",
    "I would not balance the data within the folds, as the data will not be balanced in a real-world example. Thus, the cross-validation score will not be represent the model performance well.\n",
    "\n",
    "Some ways to deal with imbalanced data is under- and over-sampling (e.g. SMOTE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Oversampling (the minority class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(sorted(Counter(y_train).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# TODO: \n",
    "* Oversampling simply with df.sample()\n",
    "* use higher fraction of df[df[\"toxic\"]==1]\n",
    "* use lower fraction of df[df[\"toxic\"]==0]\n",
    "* Bagging on some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_resampled = X_train[X_train.toxic=1].sample()\n",
    "y_resampled = y_train[X_resampled.index]\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
